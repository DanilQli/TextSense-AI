import torch
from flask import Flask, jsonify, request
from transformers import AutoTokenizer, AutoModelForSequenceClassification

app = Flask(__name__)
DEFAULT_CLASSIFICATION_PATH = '/home/lobste/assets/'
DEFAULT_ALBERT_PATH = '/home/lobste/assets/1/'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

try:
    # Load Sequence Classification Model and Tokenizer
    classification_model = AutoModelForSequenceClassification.from_pretrained(DEFAULT_CLASSIFICATION_PATH)
    classification_tokenizer = AutoTokenizer.from_pretrained(DEFAULT_CLASSIFICATION_PATH)
    classification_model.to(DEVICE)
    classification_model.eval()  # Set to evaluation mode

    # Load Albert Model and Tokenizer
    # We use AutoModel here to get the base Albert model for flexibility.
    # You might need a different AutoModelFor... class depending on the Albert model's task.
    albert_model = AutoModel.from_pretrained(DEFAULT_ALBERT_PATH)
    albert_tokenizer = AutoTokenizer.from_pretrained(DEFAULT_ALBERT_PATH)
    albert_model.to(DEVICE)
    albert_model.eval() # Set to evaluation mode
except Exception as e:
    print(f"Error processing with classification model: {e}")


@app.route('/', methods=['POST'])
def api_response():
    data = request.get_json()
    print(data)
    text = data.get('text', 'default_value')
    response_data = {}
    classification_output = None
    albert_output = None


    # --- Process with Albert Model ---
    if albert_model is None or albert_tokenizer is None or data.get('skip_albert'):
        print("Skipping Albert model or model not loaded.")
    else:
        try:
            # Load the specific model if a different path is provided
            if albert_path != DEFAULT_ALBERT_PATH:
                current_albert_model = AutoModel.from_pretrained(albert_path).to(DEVICE)
                current_albert_tokenizer = AutoTokenizer.from_pretrained(albert_path)
                current_albert_model.eval()
            else:
                current_albert_model = albert_model
                current_albert_tokenizer = albert_tokenizer

            with torch.no_grad():
                inputs_albert = current_albert_tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(DEVICE)
                outputs_albert = current_albert_model(**inputs_albert)

            # Extract a meaningful representation from the Albert output
            # For example, get the last hidden state of the [CLS] token (sentence embedding)
            albert_output = outputs_albert.last_hidden_state[:, 0, :].cpu().numpy().tolist()
            response_data['output'] = albert_output
        except Exception as e:
            print(f"Error: {e}")


        # --- Process with Sequence Classification Model ---
    if classification_model is None or classification_tokenizer is None:
        print("Skipping classification model or model not loaded.")
    else:
        try:
            # Load the specific model if a different path is provided
            if classification_path != DEFAULT_CLASSIFICATION_PATH:
                current_class_model = AutoModelForSequenceClassification.from_pretrained(classification_path).to(DEVICE)
                current_class_tokenizer = AutoTokenizer.from_pretrained(classification_path)
                current_class_model.eval()
            else:
                current_class_model = classification_model
                current_class_tokenizer = classification_tokenizer

            with torch.no_grad():
                inputs_class = current_class_tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(DEVICE)
                logits_class = current_class_model(**inputs_class).logits
                proba_class = torch.sigmoid(logits_class.detach()).cpu().numpy()[0]
                classification_output = proba_class.dot([-1, 0, 1])
            response_data['outputEmotions'] = classification_output
        except Exception as e:
            print(f"Error: {e}")
